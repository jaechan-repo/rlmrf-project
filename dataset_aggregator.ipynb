{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Dict, List, Any, Optional\n",
    "from PIL.Image import Image\n",
    "\n",
    "class Doc(TypedDict):\n",
    "    doc_id: str\n",
    "    doc_score: str\n",
    "    title: Optional[str]\n",
    "    text: Optional[str]\n",
    "\n",
    "class Question(TypedDict):\n",
    "    question_id: str\n",
    "    text: str\n",
    "    image_id: Optional[str]\n",
    "    image_file: Optional[str]\n",
    "\n",
    "class Response(TypedDict):\n",
    "    text: str\n",
    "    reward: Optional[float]\n",
    "    rid: str\n",
    "\n",
    "class DataQA(TypedDict):\n",
    "    question: Question              # Question type (e.g., text, image_file) to question\n",
    "    ref_cap: Optional[Dict]       # Caption type to caption\n",
    "    ref_ans: Dict              # List of answers, first being most relevant\n",
    "    split: str                      # \"train\" / \"dev\" / \"test\"\n",
    "    dataset: str                    # e.g, \"a-okvqa\"\n",
    "    blip_cap: Optional[str]\n",
    "    \n",
    "    docs: Optional[List[Doc]]        \n",
    "    responses: Optional[List[Response]]\n",
    "    top_response: Optional[Response]\n",
    "    misc: Optional[Dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the passages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "461148it [00:02, 174624.36it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mmfs1/gscratch/ark/chan0369/rampa-project/dataset_aggregator.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvscode/mmfs1/gscratch/ark/chan0369/rampa-project/dataset_aggregator.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m passages \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdata/contriever_msmarco/psgs_w100.tsv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvscode/mmfs1/gscratch/ark/chan0369/rampa-project/dataset_aggregator.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m passages_embeddings \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdata/contriever_msmarco/wikipedia_embeddings/passages_*\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bvscode/mmfs1/gscratch/ark/chan0369/rampa-project/dataset_aggregator.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m retriever \u001b[39m=\u001b[39m ContrieverRetriever(passages\u001b[39m=\u001b[39;49mpassages)\n",
      "File \u001b[0;32m/mmfs1/gscratch/ark/chan0369/rampa-project/rampa/passage_retriever.py:16\u001b[0m, in \u001b[0;36mContrieverRetriever.__init__\u001b[0;34m(self, passages)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, passages: \u001b[39mstr\u001b[39m):\n\u001b[1;32m     15\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoading the passages...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpassages \u001b[39m=\u001b[39m load_passages(passages)\n\u001b[1;32m     17\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpassage_id_map \u001b[39m=\u001b[39m {x[\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m]: x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpassages}\n",
      "File \u001b[0;32m/mmfs1/gscratch/ark/chan0369/rampa-project/rampa/contriever/src/data.py:244\u001b[0m, in \u001b[0;36mload_passages\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m row[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    243\u001b[0m                 ex \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m: row[\u001b[39m0\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m\"\u001b[39m: row[\u001b[39m2\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m: row[\u001b[39m1\u001b[39m]}\n\u001b[0;32m--> 244\u001b[0m                 passages\u001b[39m.\u001b[39;49mappend(ex)\n\u001b[1;32m    245\u001b[0m \u001b[39mreturn\u001b[39;00m passages\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from rampa.passage_retriever import ContrieverRetriever\n",
    "passages = \"data/contriever_msmarco/psgs_w100.tsv\"\n",
    "passages_embeddings = \"data/contriever_msmarco/wikipedia_embeddings/passages_*\"\n",
    "retriever = ContrieverRetriever(passages=passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index from data/contriever_msmarco/wikipedia_embeddings/index.faiss, meta data from data/contriever_msmarco/wikipedia_embeddings/index_meta.faiss\n",
      "Loaded index of type %s and size %d <class 'faiss.swigfaiss_avx2.IndexFlat'> 21015324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at models/contriever/ were not used when initializing Contriever: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing Contriever from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Contriever from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "retriever.prepare_model(model_path=\"models/contriever/\", \n",
    "                        passages_embeddings=passages_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions embeddings shape: torch.Size([128, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:06<00:00, 66.88s/it]\n"
     ]
    }
   ],
   "source": [
    "doc_ids = retriever.retrieve_doc_ids(['What is LLaMA?']*128,\n",
    "                            per_gpu_batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['248462',\n",
       "  '15467386',\n",
       "  '248463',\n",
       "  '14387423',\n",
       "  '248472',\n",
       "  '6419735',\n",
       "  '15467387',\n",
       "  '20273272'],\n",
       " array([1.1628287 , 1.0403614 , 1.0338705 , 1.0075382 , 0.9987637 ,\n",
       "        0.99689585, 0.99152833, 0.98600364], dtype=float32))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path = \"/mmfs1/gscratch/ark/chan0369/rampa-project/data\"\n",
    "out_path = \"/mmfs1/gscratch/ark/chan0369/rampa-project/data_prep/iter0\"\n",
    "dataset = 'aokvqa'\n",
    "split = 'train'\n",
    "in_data_path = os.path.join(in_path, dataset)\n",
    "in_file = os.path.join(in_data_path, f\"aokvqa_v1p0_{split}.json\")\n",
    "df = pd.read_json(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aokvqa conversion finished!\n",
      "s3vqa conversion finished!\n",
      "okvqa conversion finished!\n",
      "scienceqa conversion finished!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "in_path = \"/mmfs1/gscratch/ark/chan0369/rampa-project/data\"\n",
    "out_path = \"/mmfs1/gscratch/ark/chan0369/rampa-project/data_prep/iter0\"\n",
    "datasets = ['aokvqa', 's3vqa', 'okvqa', 'scienceqa']\n",
    "splits = ['train', 'val']\n",
    "\n",
    "for dataset in datasets:\n",
    "    out_data_path = os.path.join(out_path, dataset)\n",
    "    if not os.path.exists(out_data_path):\n",
    "        os.mkdir(out_data_path)\n",
    "    for split in splits:\n",
    "        out = []\n",
    "        if dataset == 'aokvqa':\n",
    "            in_file = os.path.join(in_path, dataset, f\"aokvqa_v1p0_{split}.json\")\n",
    "            df = pd.read_json(in_file)\n",
    "            df = df[~df['difficult_direct_answer']].reset_index(drop=True)\n",
    "            for _, row in df.iterrows():\n",
    "                line: DataQA = {\n",
    "                    'question': {\n",
    "                        'qid': row['question_id'], \n",
    "                        'text': row['question'], \n",
    "                        'image_id': row['image_id'],\n",
    "                        'image_file': f\"coco/{split}2017/{str(row['image_id']).zfill(12)}.jpg\",\n",
    "                    },\n",
    "                    'ref_ans': {\n",
    "                        'direct_answers': row['direct_answers'],\n",
    "                        'text': row['choices'][row['correct_choice_idx']],\n",
    "                        'rationales': row['rationales']\n",
    "                    },\n",
    "                    'ref_cap': row['rationales'], # WIP: subject to change\n",
    "                    'split': split,\n",
    "                    'dataset': dataset\n",
    "                    # 'misc': {\n",
    "                    #     'dfficult_direct_answer': row['difficult_direct_answer']\n",
    "                    # },\n",
    "                }\n",
    "                out.append(line)\n",
    "        elif dataset == 's3vqa':\n",
    "            df_questions = pd.read_json(os.path.join(in_path, dataset, f\"S3-VQA_{split if split != 'val' else 'dev'}_questions.json\"))\n",
    "            df_annotations = pd.read_json(os.path.join(in_path, dataset, f\"S3-VQA_{split if split != 'val' else 'dev'}_annotations.json\"))\n",
    "            df = df_questions.merge(df_annotations, how='inner')\n",
    "            for _, row in df.iterrows():\n",
    "                line: DataQA = {\n",
    "                    'question': {\n",
    "                        'qid': row['question_id'], \n",
    "                        'text': row['question'], \n",
    "                        'image_id': row['image_id'],\n",
    "                        'image_file': f\"openimages/s3vqa/{row['image_id']}.jpg\",\n",
    "                    },\n",
    "                    'ref_ans': {\n",
    "                        'text': row['answer']['raw'],\n",
    "                        'stem': row['answer']['answer'],\n",
    "                        'hyponym': row['hyponym'],\n",
    "                        'hypernym': row['hypernym']\n",
    "                    },\n",
    "                    'split': split,\n",
    "                    'dataset': dataset\n",
    "                }\n",
    "                out.append(line)\n",
    "        elif dataset == 'okvqa':\n",
    "            with open(os.path.join(in_path, dataset, f\"OpenEnded_mscoco_{split}2014_questions.json\")) as f:\n",
    "                df_questions = pd.DataFrame(json.load(f)['questions'])\n",
    "            with open(os.path.join(in_path, dataset, f\"mscoco_{split}2014_annotations.json\")) as f:\n",
    "                df_annotations = pd.DataFrame(json.load(f)['annotations'])\n",
    "            df = df_annotations.merge(df_questions, how='inner')\n",
    "            df = df[df['confidence'] == 5].reset_index(drop=True)\n",
    "            for _, row in df.iterrows():\n",
    "                line: DataQA = {\n",
    "                    'question': {\n",
    "                        'qid': row['question_id'], \n",
    "                        'text': row['question'], \n",
    "                        'image_id': row['image_id'],\n",
    "                        'image_file': f\"coco/{split}2017/{str(row['image_id']).zfill(12)}.jpg\",\n",
    "                    },\n",
    "                    'ref_ans': {\n",
    "                        'text': Counter([a['raw_answer'] for a in row['answers']]).most_common(1)[0][0],\n",
    "                        'stem': Counter([a['answer'] for a in row['answers']]).most_common(1)[0][0]\n",
    "                    },\n",
    "                    'misc': {\n",
    "                        'confidence': row['confidence'],\n",
    "                        'question_type': row['question_type'],\n",
    "                        'answer_type': row['answer_type'],\n",
    "                    },\n",
    "                    'split': split,\n",
    "                    'dataset': dataset\n",
    "                }\n",
    "                out.append(line)\n",
    "        elif dataset == 'scienceqa':\n",
    "            with open(os.path.join(in_path, dataset, \"problems.json\")) as f:\n",
    "                df = pd.DataFrame(json.load(f)).T\n",
    "                df = df[df['split']==split] # do not reset index!\n",
    "            choice_prefixes = [chr(ord('A') + i) for i in range(26)] # A-Z\n",
    "            def format_options(options, choice_prefixes):\n",
    "                return ' '.join([f'({c}) {o}' for c, o in zip(choice_prefixes, options)])\n",
    "            def format_prompt(r, choice_prefixes):\n",
    "                options = format_options(r['choices'], choice_prefixes)\n",
    "                # context = f\"Context: {r['hint']}\\n\" if r['hint'].strip() != \"\" else \"\"\n",
    "                # return f'''{context}Question: {r[\"question\"]}\\nOptions:{options}'''\n",
    "                return f'''{r[\"question\"]}\\nOptions: {options}'''\n",
    "            def format_label(r, choice_prefixes):\n",
    "                # letter_answer, direct_answer\n",
    "                return choice_prefixes[r['answer']], r['choices'][r['answer']]\n",
    "            for i, row in df.iterrows():\n",
    "                question_with_choices = format_prompt(row, choice_prefixes=choice_prefixes)\n",
    "                letter_answer, direct_answer = format_label(row, choice_prefixes=choice_prefixes)\n",
    "                line: DataQA = {\n",
    "                    'question': {\n",
    "                        'qid': i,\n",
    "                        'without_choices': row['question'],\n",
    "                        'choices': row['choices'],\n",
    "                        'text': question_with_choices\n",
    "                    },\n",
    "                    'ref_ans': {\n",
    "                        'index': row['answer'],\n",
    "                        'letter_answer': letter_answer,\n",
    "                        'direct_answer': direct_answer,\n",
    "                        'text': f\"{letter_answer} {direct_answer}\"\n",
    "                    },\n",
    "                    'misc': {\n",
    "                        'hint': row['hint'],\n",
    "                        'task': row['task'],\n",
    "                        'solution': row['solution'],\n",
    "                    },\n",
    "                    'split': split,\n",
    "                    'dataset': dataset\n",
    "                }\n",
    "                if row['image'] is not None:\n",
    "                    line['question']['image_file'] = f\"scienceqa/{split}/{i}\"\n",
    "                out.append(line)\n",
    "        else:\n",
    "            raise ValueError(\"dataset name not featured\")\n",
    "        out_file = os.path.join(out_data_path, f\"{split}.json\")\n",
    "        with open(out_file, 'w') as f:\n",
    "            json.dump(out, f)\n",
    "    print(f\"{dataset} conversion finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqa = pd.read_json(\"/mmfs1/gscratch/ark/chan0369/rampa-project/data/scienceqa/problems.json\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqa = sqa.T\n",
    "sqa['image'].loc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split\n",
       "train    12726\n",
       "test      4241\n",
       "val       4241\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqa['split'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A', 'West Virginia')"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choice_prefixes = [chr(ord('A') + i) for i in range(26)] # A-Z\n",
    "def format_options(options, choice_prefixes):\n",
    "    return ' '.join([f'({c}) {o}' for c, o in zip(choice_prefixes, options)])\n",
    "def format_prompt(r, choice_prefixes):\n",
    "    options = format_options(r['choices'], choice_prefixes)\n",
    "    # context = f\"Context: {r['hint']}\\n\" if r['hint'].strip() != \"\" else \"\"\n",
    "    # return f'''{context}Question: {r[\"question\"]}\\nOptions:{options}'''\n",
    "    return f'''{r[\"question\"]}\\nOptions: {options}'''\n",
    "def format_label(r, choice_prefixes):\n",
    "    return choice_prefixes[r['answer']], r['choices'][r['answer']]\n",
    "format_label(row, choice_prefixes=choice_prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answer\n",
       "1    8542\n",
       "0    8399\n",
       "2    2961\n",
       "3    1275\n",
       "4      31\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqa['answer'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219      Which bird's beak is also adapted to get necta...\n",
       "20021            Which wax candle has more thermal energy?\n",
       "5912     Compare the average kinetic energies of the pa...\n",
       "17250                        Which is a complete sentence?\n",
       "10129    Compare the motion of two blue jays. Which blu...\n",
       "Name: question, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_samples['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = os.path.join(data_path, 'aokvqa', 'aokvqa_v1p0_train.json')\n",
    "dataset = pd.read_json(sample_path)\n",
    "split = 'train'\n",
    "row = dataset.loc[0]\n",
    "line: DataQA = {\n",
    "    'question': {\n",
    "        'text': row['question'],\n",
    "        'qid': row['question_id'],\n",
    "        'image_file': f\"coco/{split}2017/{str(row['image_id']).zfill(12)}.jpg\"\n",
    "    },\n",
    "    'ref_cap': {\n",
    "        'rationales': row['rationales']\n",
    "    },\n",
    "    'ref_ans': {\n",
    "        'direct_answers': row['direct_answers']\n",
    "    },\n",
    "    'misc': {\n",
    "        'dfficult_direct_answer': row['difficult_direct_answer']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': {'qid': 3281015,\n",
       "  'text': 'What are the people riding?',\n",
       "  'image_id': 328101,\n",
       "  'image_file': 'coco/val2017/000000328101.jpg'},\n",
       " 'ref_ans': {'raw': 'elephants', 'answer': 'elephant'},\n",
       " 'misc': {'confidence': 5, 'question_type': 'eight', 'answer_type': 'other'}}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
